{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fb4f7b52-5628-4d15-ac5b-45ddec3c2254",
   "metadata": {},
   "source": [
    "# Building a Decision Tree from Scratch: A Visual Guide\n",
    "\n",
    "## What Are We Building?\n",
    "\n",
    "Imagine you're sorting fruit. You might ask: \"Is it red?\" If yes, it's probably an apple. If no, ask: \"Is it yellow?\" If yes, maybe a banana. This chain of yes/no questions is exactly how a decision tree works—it's an algorithm that learns to ask the right questions to classify data.\n",
    "\n",
    "In this guide, we'll build a decision tree classifier from the ground up, understanding each piece as we go.\n",
    "\n",
    "---\n",
    "\n",
    "## Part 1: Measuring \"Messiness\" with Gini Impurity\n",
    "\n",
    "Before we can build our tree, we need a way to measure how \"mixed up\" our data is. Enter **Gini impurity**.\n",
    "\n",
    "### The Intuition\n",
    "\n",
    "Imagine you have a bag of colored marbles:\n",
    "- **Bag A**: All red marbles → Very \"pure\" (Gini = 0)\n",
    "- **Bag B**: 50% red, 50% blue → Very \"messy\" (Gini = 0.5)\n",
    "- **Bag C**: 90% red, 10% blue → Mostly pure (Gini = 0.18)\n",
    "\n",
    "Gini impurity gives us a number that tells us how mixed our labels are. The formula is:\n",
    "\n",
    "**Gini = 1 - Σ(pᵢ²)**\n",
    "\n",
    "Where pᵢ is the probability of each class.\n",
    "\n",
    "### The Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "124586cd-a24e-40b4-a280-ea7cb6c42dbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gini(y):\n",
    "    \"\"\"Calculate how mixed up our labels are\"\"\"\n",
    "    if len(y) == 0:\n",
    "        return 0\n",
    "    \n",
    "    # Count each label\n",
    "    counts = {}\n",
    "    for label in y:\n",
    "        counts[label] = counts.get(label, 0) + 1\n",
    "    \n",
    "    # Convert counts to probabilities\n",
    "    total = len(y)\n",
    "    probabilities = [count / total for count in counts.values()]\n",
    "    \n",
    "    # Calculate impurity\n",
    "    impurity = 1 - sum(p ** 2 for p in probabilities)\n",
    "    \n",
    "    return impurity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36d36c38-a4f6-4929-86bf-fe1fe2be9d17",
   "metadata": {},
   "source": [
    "### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "95b0e44d-604d-4b95-aa66-d6856e4600ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "pure_labels = [0, 0, 0, 0]      # Gini = 0 (perfect!)\n",
    "mixed_labels = [0, 1, 0, 1]     # Gini = 0.5 (messy!)\n",
    "mostly_pure = [0, 0, 0, 1]      # Gini = 0.375 (pretty good)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b81fb459-1ec1-440c-9b9c-f1e1d7827003",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "0.5\n",
      "0.375\n"
     ]
    }
   ],
   "source": [
    "print(gini(pure_labels))\n",
    "print(gini(mixed_labels))\n",
    "print(gini(mostly_pure))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a688e379-cf60-4667-89be-b5db97ea23e6",
   "metadata": {},
   "source": [
    "## Part 2: Finding the Best Split\n",
    "\n",
    "Now that we can measure messiness, how do we decide where to split our data? We try every possible split and pick the one that reduces messiness the most.\n",
    "\n",
    "### The Strategy\n",
    "\n",
    "Think of it like organizing a messy closet. You could organize by:\n",
    "- Color (shirts on left, pants on right)\n",
    "- Season (summer on left, winter on right)\n",
    "- Formality (casual vs. formal)\n",
    "\n",
    "We try all options and pick the one that makes the most sense (reduces chaos the most).\n",
    "\n",
    "### Gini Gain\n",
    "\n",
    "**Gini Gain** = How much messiness did we remove?\n",
    "\n",
    "```\n",
    "Gain = (Messiness Before) - (Weighted Average Messiness After)\n",
    "```\n",
    "\n",
    "### The Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1b854442-0c7f-410f-9e44-cf4a0a878078",
   "metadata": {},
   "outputs": [],
   "source": [
    "def best_split(X, y):\n",
    "    \"\"\"Try every possible split and return the best one\"\"\"\n",
    "    best_gain = 0\n",
    "    best_feature = None\n",
    "    best_threshold = None\n",
    "    current_impurity = gini(y)\n",
    "    \n",
    "    # Try splitting on each feature\n",
    "    for feature in range(len(X[0])):\n",
    "        # Try each unique value as a threshold\n",
    "        thresholds = set(row[feature] for row in X)\n",
    "        \n",
    "        for threshold in thresholds:\n",
    "            # Split data: left (≤ threshold) and right (> threshold)\n",
    "            left_y = [y[i] for i in range(len(y)) if X[i][feature] <= threshold]\n",
    "            right_y = [y[i] for i in range(len(y)) if X[i][feature] > threshold]\n",
    "            \n",
    "            # Skip if split leaves one side empty\n",
    "            if len(left_y) == 0 or len(right_y) == 0:\n",
    "                continue\n",
    "            \n",
    "            # Calculate weighted impurity of the split\n",
    "            left_impurity = gini(left_y)\n",
    "            right_impurity = gini(right_y)\n",
    "            weighted_impurity = (len(left_y) / len(y)) * left_impurity + \\\n",
    "                               (len(right_y) / len(y)) * right_impurity\n",
    "            \n",
    "            # How much did we improve?\n",
    "            gain = current_impurity - weighted_impurity\n",
    "            \n",
    "            # Keep track of the best split so far\n",
    "            if gain > best_gain:\n",
    "                best_gain = gain\n",
    "                best_feature = feature\n",
    "                best_threshold = threshold\n",
    "    \n",
    "    return best_feature, best_threshold, best_gain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a9a85cf-0971-4daa-bad4-ffe00981a69e",
   "metadata": {},
   "source": [
    "### Visual Example\n",
    "\n",
    "```\n",
    "Before split: [0, 0, 1, 1, 0, 1]  (Gini = 0.5)\n",
    "\n",
    "Try: Feature_0 <= 2.5\n",
    "├─ Left:  [0, 0, 0]  (Gini = 0)    ← Pure!\n",
    "└─ Right: [1, 1, 1]  (Gini = 0)    ← Pure!\n",
    "\n",
    "Gain = 0.5 - 0 = 0.5  ← Excellent split!\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fca419e6-8308-4a57-965f-fa097411af33",
   "metadata": {},
   "source": [
    "## Part 3: Building the Tree Recursively\n",
    "\n",
    "Now comes the magic. We'll build our tree by recursively splitting the data until we can't (or shouldn't) split anymore.\n",
    "\n",
    "### When to Stop Splitting\n",
    "\n",
    "We stop when:\n",
    "1. **All labels are the same** (pure node—nothing to gain)\n",
    "2. **We've gone too deep** (max_depth reached—prevent overfitting)\n",
    "3. **Too few samples** (not enough data to make a reliable split)\n",
    "4. **No gain from splitting** (can't improve anymore)\n",
    "\n",
    "### The Code\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c988a10f-b9a8-4e12-bcf7-9a87245ac05f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def most_common_label(y):\n",
    "    \"\"\"Return the most frequent label\"\"\"\n",
    "    counts = {}\n",
    "    for label in y:\n",
    "        counts[label] = counts.get(label, 0) + 1\n",
    "    return max(counts.items(), key=lambda x: x[1])[0]\n",
    "\n",
    "\n",
    "def build_tree(X, y, depth=0, max_depth=5, min_samples_leaf=1):\n",
    "    \"\"\"Recursively build the decision tree\"\"\"\n",
    "    \n",
    "    # STOPPING CONDITION 1: Pure node\n",
    "    if gini(y) == 0:\n",
    "        return {\n",
    "            'leaf': True,\n",
    "            'class': y[0],\n",
    "            'samples': len(y),\n",
    "            'impurity': 0\n",
    "        }\n",
    "    \n",
    "    # STOPPING CONDITION 2: Max depth reached\n",
    "    if depth >= max_depth:\n",
    "        return {\n",
    "            'leaf': True,\n",
    "            'class': most_common_label(y),\n",
    "            'samples': len(y),\n",
    "            'impurity': gini(y)\n",
    "        }\n",
    "    \n",
    "    # STOPPING CONDITION 3: Too few samples\n",
    "    if len(y) < 2 * min_samples_leaf:\n",
    "        return {\n",
    "            'leaf': True,\n",
    "            'class': most_common_label(y),\n",
    "            'samples': len(y),\n",
    "            'impurity': gini(y)\n",
    "        }\n",
    "    \n",
    "    # Find the best way to split\n",
    "    feature, threshold, gain = best_split(X, y)\n",
    "    \n",
    "    # STOPPING CONDITION 4: No improvement\n",
    "    if gain == 0 or feature is None:\n",
    "        return {\n",
    "            'leaf': True,\n",
    "            'class': most_common_label(y),\n",
    "            'samples': len(y),\n",
    "            'impurity': gini(y)\n",
    "        }\n",
    "    \n",
    "    # Split the data\n",
    "    left_indices = [i for i in range(len(y)) if X[i][feature] <= threshold]\n",
    "    right_indices = [i for i in range(len(y)) if X[i][feature] > threshold]\n",
    "    \n",
    "    # Check minimum samples constraint\n",
    "    if len(left_indices) < min_samples_leaf or len(right_indices) < min_samples_leaf:\n",
    "        return {\n",
    "            'leaf': True,\n",
    "            'class': most_common_label(y),\n",
    "            'samples': len(y),\n",
    "            'impurity': gini(y)\n",
    "        }\n",
    "    \n",
    "    # Recursively build left and right subtrees\n",
    "    left_X = [X[i] for i in left_indices]\n",
    "    left_y = [y[i] for i in left_indices]\n",
    "    right_X = [X[i] for i in right_indices]\n",
    "    right_y = [y[i] for i in right_indices]\n",
    "    \n",
    "    return {\n",
    "        'leaf': False,\n",
    "        'feature': feature,\n",
    "        'threshold': threshold,\n",
    "        'samples': len(y),\n",
    "        'impurity': gini(y),\n",
    "        'gain': gain,\n",
    "        'left': build_tree(left_X, left_y, depth + 1, max_depth, min_samples_leaf),\n",
    "        'right': build_tree(right_X, right_y, depth + 1, max_depth, min_samples_leaf)\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0980a50b-282e-4ee0-b636-131749f13c9a",
   "metadata": {},
   "source": [
    "## Part 4: Making Predictions\n",
    "\n",
    "Once we've built our tree, making predictions is like following a flowchart.\n",
    "\n",
    "### The Process\n",
    "\n",
    "1. Start at the root (top) of the tree\n",
    "2. Check the feature value of your sample\n",
    "3. Go left if value ≤ threshold, right if value > threshold\n",
    "4. Repeat until you reach a leaf\n",
    "5. Return the leaf's class prediction\n",
    "\n",
    "### The Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "bea38776-05f5-4ce1-a75f-ea7797003495",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_single(tree, sample):\n",
    "    \"\"\"Make a prediction for one sample by walking down the tree\"\"\"\n",
    "    \n",
    "    # Base case: we've reached a leaf node\n",
    "    if tree['leaf']:\n",
    "        return tree['class']\n",
    "    \n",
    "    # Recursive case: keep going left or right\n",
    "    if sample[tree['feature']] <= tree['threshold']:\n",
    "        return predict_single(tree['left'], sample)\n",
    "    else:\n",
    "        return predict_single(tree['right'], sample)\n",
    "\n",
    "\n",
    "def predict(tree, X):\n",
    "    \"\"\"Make predictions for multiple samples\"\"\"\n",
    "    return [predict_single(tree, sample) for sample in X]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff190197-17b9-4157-a51d-b49503c6bb30",
   "metadata": {},
   "source": [
    "### Visual Example\n",
    "\n",
    "```\n",
    "Sample: [2.5, 3.0]\n",
    "\n",
    "         [Feature_0 <= 3.0]\n",
    "         /                \\\n",
    "    YES (2.5 ≤ 3.0)        NO\n",
    "       /                      \\\n",
    "  Predict: 0             Predict: 1\n",
    "      ↑\n",
    "   We land here!\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f52e0d53-472a-4a12-a9c7-e63848a492d0",
   "metadata": {},
   "source": [
    "## Part 5: Putting It All Together\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "d71d3759-e6cf-4400-ab34-7a9ace9b6747",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TREE STRUCTURE:\n",
      "[Feature_0 <= 3.00] (samples=6, impurity=0.500, gain=0.250)\n",
      "  Left:\n",
      "    [Feature_0 <= 1.00] (samples=4, impurity=0.375, gain=0.375)\n",
      "      Left:\n",
      "        → Predict: 1 (samples=1, impurity=0.000)\n",
      "      Right:\n",
      "        → Predict: 0 (samples=3, impurity=0.000)\n",
      "  Right:\n",
      "    → Predict: 1 (samples=2, impurity=0.000)\n",
      "Sample 0: [2.5, 1.5] → True: 0, Pred: 0 ✓\n",
      "Sample 1: [3.5, 2.5] → True: 1, Pred: 1 ✓\n",
      "Sample 2: [1.5, 3.5] → True: 0, Pred: 0 ✓\n",
      "Sample 3: [4.5, 4.5] → True: 1, Pred: 1 ✓\n",
      "Sample 4: [3.0, 1.0] → True: 0, Pred: 0 ✓\n",
      "Sample 5: [1.0, 4.0] → True: 1, Pred: 1 ✓\n",
      "\n",
      "Accuracy: 100.00%\n"
     ]
    }
   ],
   "source": [
    "# Sample data: 6 points with 2 features each\n",
    "X = [\n",
    "    [2.5, 1.5],  # Class 0\n",
    "    [3.5, 2.5],  # Class 1\n",
    "    [1.5, 3.5],  # Class 0\n",
    "    [4.5, 4.5],  # Class 1\n",
    "    [3.0, 1.0],  # Class 0\n",
    "    [1.0, 4.0]   # Class 1\n",
    "]\n",
    "y = [0, 1, 0, 1, 0, 1]\n",
    "\n",
    "# Build the tree\n",
    "tree = build_tree(X, y, max_depth=3, min_samples_leaf=1)\n",
    "\n",
    "# Visualize it\n",
    "print(\"TREE STRUCTURE:\")\n",
    "print_tree(tree, [\"Feature_0\", \"Feature_1\"])\n",
    "\n",
    "# Make predictions\n",
    "predictions = predict(tree, X)\n",
    "\n",
    "# Check accuracy\n",
    "for i, (sample, true, pred) in enumerate(zip(X, y, predictions)):\n",
    "    match = \"✓\" if true == pred else \"✗\"\n",
    "    print(f\"Sample {i}: {sample} → True: {true}, Pred: {pred} {match}\")\n",
    "\n",
    "accuracy = sum(1 for t, p in zip(y, predictions) if t == p) / len(y)\n",
    "print(f\"\\nAccuracy: {accuracy:.2%}\")\n",
    "\n",
    "# Test on new data\n",
    "new_samples = [[2.0, 2.0], [4.0, 4.0], [1.0, 1.0]]\n",
    "new_predictions = predict(tree, new_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a46296a1-ab39-4ac9-b619-2286e3b83635",
   "metadata": {},
   "source": [
    "## Understanding the Hyperparameters\n",
    "\n",
    "### max_depth\n",
    "- **What it does**: Limits how many questions the tree can ask\n",
    "- **Small value (e.g., 1-2)**: Simple tree, may underfit\n",
    "- **Large value (e.g., 10+)**: Complex tree, may overfit\n",
    "- **Sweet spot**: Usually 3-7 for small datasets\n",
    "\n",
    "### min_samples_leaf\n",
    "- **What it does**: Requires each leaf to have at least N samples\n",
    "- **Small value (e.g., 1)**: More specific predictions, risk of overfitting\n",
    "- **Large value (e.g., 5+)**: More general predictions, more robust\n",
    "- **Use case**: Increase this if your tree is too complex\n",
    "\n",
    "---\n",
    "\n",
    "## Key Takeaways\n",
    "\n",
    "1. **Decision trees work by asking questions**: Each node tests a feature against a threshold\n",
    "2. **Gini impurity measures messiness**: Lower is better (0 = pure)\n",
    "3. **We split to maximize information gain**: Pick the split that best separates classes\n",
    "4. **Recursion builds the tree**: Split, then split the splits, until we should stop\n",
    "5. **Hyperparameters control complexity**: Use them to prevent overfitting\n",
    "\n",
    "---\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "Now that you understand the basics, you can:\n",
    "- Add support for continuous target variables (regression trees)\n",
    "- Implement pruning to simplify overfitted trees\n",
    "- Build a random forest (multiple trees voting together)\n",
    "- Add feature importance calculations\n",
    "- Handle missing values\n",
    "\n",
    "Happy tree building!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "652784aa-6fc6-4ec9-bfe0-5adea77d6072",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
